# Project Samarth - LLM Prompts and API Calls Documentation

## üö® Important Note
**The system currently does NOT have any API key configured and is running in fallback mode without actual LLM calls.**

---

## üìù LLM Prompts in Use

### 1. Query Generation Prompt
**Location**: `query_generator.py` - `generate_query()` method
**Purpose**: Convert natural language questions to pandas DataFrame queries

```python
prompt = f"""
You are an expert data analyst working with Indian agricultural and climate data. 
Your task is to generate pandas DataFrame queries to answer user questions.

DATABASE SCHEMA:
{schema_context}

USER QUESTION: {question}

INSTRUCTIONS:
1. Analyze the question to identify which datasets are relevant
2. Generate pandas code that queries the appropriate DataFrames
3. Use proper pandas syntax (df.query(), df.groupby(), df.sort_values(), etc.)
4. Limit results to top {max_results} rows using .head({max_results})
5. Include proper filtering, grouping, and sorting as needed
6. Handle potential data type issues (string matching, case sensitivity)
7. Use exact column names from the schema

IMPORTANT:
- Use the exact DataFrame names from the schema
- Use exact column names as shown in the schema
- For string matching, use .str.contains() or .str.lower() for case-insensitive search
- Always include .head({max_results}) to limit results
- Return only the pandas code, no explanations

EXAMPLE OUTPUT FORMAT:
```python
# Query for mandis in specific state
mandis_df = data_loader.get_dataframe('agmark_mandis_and_locations')
result = mandis_df[mandis_df['State Name'].str.contains('Punjab', case=False)].head({max_results})
```

Generate the pandas query code:
"""
```

### 2. Final Answer Generation Prompt
**Location**: `qa_system.py` - `_generate_final_answer()` method
**Purpose**: Generate comprehensive answers from query results

```python
prompt = f"""
You are an expert agricultural data analyst. Based on the query results below, provide a comprehensive answer to the user's question.

USER QUESTION: {question}

QUERY EXECUTED:
{query_code}

QUERY RESULTS:
{formatted_data}

INSTRUCTIONS:
1. Analyze the data and provide a clear, accurate answer to the question
2. Include specific numbers, names, and details from the data
3. If the data shows trends or patterns, mention them
4. If the question asks for comparisons, make them explicit
5. Be concise but comprehensive
6. Always cite specific data points from the results
7. If the data is insufficient to fully answer the question, explain what information is available and what's missing

Provide your answer:
"""
```

---

## üîß LLM API Calls

### 1. Query Generation Call
**Location**: `query_generator.py` - `generate_query()` method
**Model**: GPT-3.5-turbo
**Parameters**:
```python
response = self.client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": prompt}],
    temperature=0.1,
    max_tokens=1000
)
```

### 2. Answer Generation Call
**Location**: `qa_system.py` - `_generate_final_answer()` method
**Model**: GPT-3.5-turbo
**Parameters**:
```python
response = self.query_generator.client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": prompt}],
    temperature=0.3,
    max_tokens=1000
)
```

---

## ‚ö†Ô∏è Current Implementation Status

### What's Actually Happening
1. **No API Key**: System runs without OpenAI API key
2. **Fallback Mode**: Uses rule-based query generation instead of LLM
3. **No Real LLM Calls**: All responses are generated by fallback functions

### Fallback Functions
1. **`_generate_fallback_query()`**: Rule-based query generation using keyword matching
2. **`_generate_fallback_answer()`**: Simple data summary without LLM processing

### Code Evidence
```python
# In query_generator.py
if OPENAI_AVAILABLE and api_key:
    self.client = openai.OpenAI(api_key=api_key)
else:
    self.client = None  # No LLM client

# In qa_system.py
if OPENAI_AVAILABLE and self.query_generator.client:
    # Make actual LLM call
else:
    # Use fallback function
    return self._generate_fallback_answer(question, formatted_data)
```

---

## üîë API Key Configuration

### Current Setup
- **Environment Variable**: `OPENAI_API_KEY` (not set)
- **User Input**: Streamlit sidebar input (optional)
- **Default Behavior**: Fallback mode when no key provided

### Required for LLM Functionality
```bash
export OPENAI_API_KEY="your-api-key-here"
```

Or provide in Streamlit interface when prompted.

---

## üìä Prompt Context Data

### Schema Context
The system includes comprehensive database schema information in prompts:
- **Column Names**: All 6 datasets with column lists
- **Data Types**: Pandas dtypes for each column
- **Sample Data**: First 3 rows of each dataset
- **Unique Counts**: Number of unique values per column
- **Null Counts**: Missing value statistics

### Example Schema Context
```
=== AGMARK_MANDIS_AND_LOCATIONS ===
Shape: (4062, 6)
Columns: ['Mandi Name - Agmark', 'Mandi Name (Hi)', 'District ID', 'District Name - Agmark', 'State Name', 'Source']
Data Types: {'Mandi Name - Agmark': dtype('O'), 'Mandi Name (Hi)': dtype('O'), ...}
Sample Data: [{"Mandi Name - Agmark": "Car Nicobar", ...}]
Unique Values per Column: {'Mandi Name - Agmark': 4062, ...}
```

---

## üéØ Summary

**Current State**: System is running in fallback mode without actual LLM calls
**LLM Integration**: Ready but requires API key configuration
**Prompts**: Well-structured for agricultural data analysis
**Fallback**: Functional rule-based system for basic queries

To enable full LLM functionality, provide an OpenAI API key either via environment variable or Streamlit interface.
